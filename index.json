[{"content":"What exactly are the LSTM layer, cell, neuron, unit? The number of units is the number of neurons in a layer. The number of units defines the dimension of the hidden states. The dimension of the cell state is the same as that of the hidden state. In the following, I will use $h$ for the length of hidden states \u0026ndash; the number of hidden units. It\u0026rsquo;s called hidden_size in PyTorch and num_units in Tensorflow.\nThe colah blog shows the core idea behind LSTMs nicely illustrating the 4 gates inside a LSTM neuron.\nFig. 1. LSTM neuron. (Image source: colah blog) When coding a LSTM model, how do we count the number of parameters in the matrix multiplication? A LSTM cell contains $h$ LSTM neurons.\nFig. 2. LSTM cell. (Image source: Raimi Karim) The 4 gates suggest that a LSTM has 4 feed-forward neural networks. If we have a sample input of feature size $i$, we concatenate the sample input with the hidden state and then pass them ($i+h$) together to the gates. The number of parameters in this LSTM is calculated as: $$ 4 \\times [h(h+i) + h] $$\nThis is the number of parameters in one LSTM cell, a.k.a. one LSTM layer.\n","permalink":"https://rongrg.github.io/posts/2023-02-10-lstm/","summary":"What exactly are the LSTM layer, cell, neuron, unit? The number of units is the number of neurons in a layer. The number of units defines the dimension of the hidden states. The dimension of the cell state is the same as that of the hidden state. In the following, I will use $h$ for the length of hidden states \u0026ndash; the number of hidden units. It\u0026rsquo;s called hidden_size in PyTorch and num_units in Tensorflow.","title":"LSTM, a special RNN"},{"content":"Setting up an ML envronment can be a tricky thing. Here\u0026rsquo;s what worked for me on how to set up the environment and keep track of experiments.\nProject Setup Directory Structure Cookiecuttet\nComputational Environment Pycharm virtualenv pip install the packages: [1] Unzip the downloaded mjpro150 into ~/.mujoco/mjpro150, and place the mjkey.txt file at ~/.mujoco/mjkey.txt.\n[2] Run pip3 install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[3] Remove ~/.mujoco/mjpro150/bin/libglfw.3.dylib\n[4] Run brew install llvm boost hdf5 glfw\n[5] Add\nexport PATH=\u0026#34;/usr/local/opt/llvm/bin:$PATH\u0026#34; export CC=\u0026#34;/usr/local/opt/llvm/bin/clang\u0026#34; export CXX=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX11=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX14=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX17=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX1X=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export LDFLAGS=\u0026#34;-L/usr/local/opt/llvm/lib\u0026#34; export CPPFLAGS=\u0026#34;-I/usr/local/opt/llvm/include\u0026#34; to .zshrc and source ~/.zshrc\n[6] Run pip install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[7] Run python3 -c 'import mujoco_py'\n[8] Run pip install 'gym[all]'\nExperiments Hyperparameters Description Performance on benchmarking tasks Preparing for publishing README Add packages to requirements.txt pip freeze \u0026gt; requirements.txt Source code Visualizations ","permalink":"https://rongrg.github.io/posts/2022-05-24-blog-post-1/","summary":"Setting up an ML envronment can be a tricky thing. Here\u0026rsquo;s what worked for me on how to set up the environment and keep track of experiments.\nProject Setup Directory Structure Cookiecuttet\nComputational Environment Pycharm virtualenv pip install the packages: [1] Unzip the downloaded mjpro150 into ~/.mujoco/mjpro150, and place the mjkey.txt file at ~/.mujoco/mjkey.txt.\n[2] Run pip3 install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[3] Remove ~/.mujoco/mjpro150/bin/libglfw.3.dylib\n[4] Run brew install llvm boost hdf5 glfw","title":"How to Set Up an RL Project Less Complicated?"},{"content":"","permalink":"https://rongrg.github.io/content/_index.md","summary":"","title":""},{"content":"Summer 2018, Summer 2022, Winter 2022\n","permalink":"https://rongrg.github.io/teaching/2022-winter-teaching-4/","summary":"Summer 2018, Summer 2022, Winter 2022","title":"Advanced Topics in Reinforcement Learning"},{"content":"Summer 2019, Summer 2020, Summer 2021, Summer 2022\n","permalink":"https://rongrg.github.io/teaching/2019-summer-teaching-2/","summary":"Summer 2019, Summer 2020, Summer 2021, Summer 2022","title":"Data Structure and Algorithms"},{"content":"Winter 2018, Winter 2019, Winter 2020, Winter 2021\n","permalink":"https://rongrg.github.io/teaching/2018-winter-teaching-1/","summary":"Winter 2018, Winter 2019, Winter 2020, Winter 2021","title":"Introduction to Computer Science and Programming"},{"content":" Risk Sensitivity under Partially Observable Markov Decision Processes\nNikolas Höft, Rong Guo, Vaios Laschos, Sein Jeung, Dirk Ostwald und Klaus Obermayer\nConference on Cognitive Computational Neuroscience (CCN), 2019 paper\nInteraction of instrumental and goal-directed learning modulates prediction error representations in the ventral striatum\nRong Guo, Wendelin Böhmer, Martin Hebart, Samson Chien, Tobias Sommer, Klaus Obermayer and Jan Gläscher\nJournal of Neuroscience, 2016 paper\nAltered behavioral and neural responsiveness to counterfactual gains in the elderly\nMichael J Tobia, Rong Guo, Jan Gläscher, Ulrike Schwarze, Stefanie Brassen, Christian Büchel, Klaus Obermayer and Tobias Sommer\nCognitive, Affective, \u0026amp; Behavioral Neuroscience, 2016 paper\nNon-deterministic policy improvement stabilizes approximated reinforcement learning\nWendelin Böhmer, Rong Guo and Klaus Obermayer\nEuropean Workshop on Reinforcement Learning (EWRL), 2016 paper\nNeural systems for choice and valuation with counterfactual learning signals\nMichael J Tobia (co-first author), Rong Guo (co-first author), Ulrike Schwarze, Wendelin Böhmer, Jan Gläscher, Barbara Finckh, Andreas Marschner, Christian Büchel, Klaus Obermayer and Tobias Sommer NeuroImage, 2014 paper\nThis listing can also be found on my Google Scholar profile.\n","permalink":"https://rongrg.github.io/publications/","summary":"Risk Sensitivity under Partially Observable Markov Decision Processes\nNikolas Höft, Rong Guo, Vaios Laschos, Sein Jeung, Dirk Ostwald und Klaus Obermayer\nConference on Cognitive Computational Neuroscience (CCN), 2019 paper\nInteraction of instrumental and goal-directed learning modulates prediction error representations in the ventral striatum\nRong Guo, Wendelin Böhmer, Martin Hebart, Samson Chien, Tobias Sommer, Klaus Obermayer and Jan Gläscher\nJournal of Neuroscience, 2016 paper\nAltered behavioral and neural responsiveness to counterfactual gains in the elderly","title":"Publications"},{"content":" What is risk? Generally, risk might arise whenever there is uncertainty. In a financial situation, investment risk can be identified with uncertain monetary loss. In a safety-critical engineering system, risk is the undesirable detrimental outcome.\nHow do we measure risk? Risk metrics typically involve probability distribution of outcomes and quantified loss for each outcome.\nThe standard deviation is a measure of uncertainty, but not a good choice for risk measure. Risk concerns only losses, but standard deviation can\u0026rsquo;t account for such asymmetric nature.\nArtzner et. al. (1998) provided an axiomatic definition that a risk measure should satisfy, which they call coherent risk measure. They denote the risk measure by the functional $\\rho(X)$, which assigns a real-valued number to a random variable $X$. $X$ can be thought of as payoffs or returns.\nMonotonicity: $\\rho(Y) \\leq \\rho(X)$ if $Y \\geq X$. Positive Homogeneity: $\\rho(0) = 0, \\rho(\\lambda X) = \\lambda \\rho(X)$ for all $X$ and all $\\lambda \u0026gt; 0$. Sub-additivity: $\\rho(X+Y) \\leq \\rho(X) + \\rho(Y)$ for all $X$ and $Y$. Translation invariance: $\\rho(X+Y) = \\rho(X) -C$ for all $X$ and $C \\in \\mathbb{R}$. The positive Homogeneity and sub-additivity imply that the functional is convex, $$ \\rho(\\lambda X+ (1-\\lambda)Y) \\leq \\rho(\\lambda X) + \\rho((1-\\lambda)Y) = \\lambda \\rho(X) + (1-\\lambda) \\rho(Y) $$ where $\\lambda \\in [0,1]$. This convexity property supports the diversification effect: The risk of a portfolio is no greater than the sum of the risks of its constituents.\nThe translation invariance shows the minimum amount of capital requirements to hedge the risk $\\rho(X)$, $\\rho(X+\\rho(X)) = 0$.\nValue at risk (VaR) Conditional value at risk (CVaR) A commonly used risk measure is the expected shortfall, also known as the conditional value at risk, i.e., CVaR.\nEntropic value at risk (EVaR) Risk-sensitive reinforcement learning Quantiles Let $Z$ be a continuous random variable with a probability density $f$, the cumulative distribution function $F$, and $\\tau \\in (0, 1)$, then the $τ$-th quantile of $Z$, $z_{\\tau}$ is the inverse of its c.d.f. $$z_{\\tau} = F^{-1}(\\tau), ~~ \\tau = F(z_{\\tau}) = \\int_{-\\infty}^{z_{\\tau}}f(z)\\mathop{\\mathrm{d}}z.$$ We have $$\\tau = F(z), ~~ \\mathop{\\mathrm{d}}\\tau = F\u0026rsquo;(z)\\mathop{\\mathrm{d}}z = f(z)\\mathop{\\mathrm{d}}z $$ $$ \\int_0^1 F^{-1}(\\tau)\\mathop{\\mathrm{d}}\\tau = \\int_{\\infty}^{\\infty}F^{-1}(F(z))F\u0026rsquo;(z)\\mathop{\\mathrm{d}}z = \\int_{\\infty}^{\\infty} zf(z)\\mathop{\\mathrm{d}}z = \\mathbb{E}(z) $$\nQuantile Regression sklearn.linear_model.QuantileRegressor is a method for estimating $F^{-1}(\\tau)$\nAs a linear model, the QuantileRegressor gives linear predictions $\\hat{z}(w, X) = Xw$ for the $\\tau$-th quantile ($X:= (x,a)$). The weights $w$ are then found by the following minimization problem: $$\\min_{w} {\\frac{1}{n_{\\text{samples}}} \\sum_i PB_{\\tau}(z_i - X_i w) + \\alpha ||w||_1}. $$ The quantile regression loss (a.k.a pinball loss or linear loss) is\n\\begin{split}PB_{\\tau}(u) = |\\tau - \\mathbb{I}_{u \\leq 0}| u = \\tau \\max(u, 0) + (1 - \\tau) \\max(-u, 0) = \\begin{cases} \\tau u, \u0026amp; u \u0026gt; 0, \\ 0, \u0026amp; u = 0, \\ (\\tau-1) u, \u0026amp; u \u0026lt; 0 \\end{cases}\\end{split}\nImplicit Quantile Network is a deterministic neural network parameterizes $(x, a, \\tau)$ and output the quantile of the target distribution $z_{\\tau} = F^{-1}(\\tau) \\approx Q_{\\theta} (x, a; \\tau)$\nThe IQN loss minimizes sampled errors for $Z$ at different $\\tau$s:\nSample $\\tau_i \\sim U([0,1])$ for $N$ times, $i = 1, \\dots, N$ $\\mathcal{L}(x) = \\frac{1}{N}\\sum_i PB_{\\tau_i}(z_{\\tau_i} - Q_{\\theta} (x, a; \\tau_i))$ (If the entire target distribution $Z$ is not accessible, $z_{\\tau_i}$ is replaced with a single target scalar z as an approximation.) Instead of the PB loss, Dabney et al., 2018 used the Huber quantile regression loss with threshold $\\kappa$: \\begin{split}\\rho_{\\tau}^{\\kappa}(u) = \\begin{cases} |\\tau - \\mathbb{I}{u \\leq 0}|\\frac{u^2}{2\\kappa}, \u0026amp; |u| \\leq \\kappa, \\ |\\tau - \\mathbb{I}{u \\leq 0}|(|u|-\\frac{1}{2}\\kappa), \u0026amp; |u| \u0026gt; \\kappa \\end{cases}\\end{split}\nPolicy The advantage of estimating $Z$ through randomly sampled $\\tau$: risk-sensitive policy.\nrisk refers to the uncertainty over possible outcomes and risk-sensitive policies are those which depend upon more than the mean of the outcomes. Here, the uncertainty is captured by the distribution over returns. Let\u0026rsquo;s take a simple illustrative risk-sensitive criterion Javier Garćıa and Fernando Ferńandez, 2015: $$ \\max_{\\pi}(\\mathbb{E}(z) - \\beta \\mathrm{Var}(z)) $$ Risk-neutral policy $$ \\pi(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[z] = \\frac{1}{N}\\sum_i Q_{\\theta}(x, a; \\tau_i) $$\nRisk-sensitive Policy $$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[U(z)], $$ which is equivalent to maximizing a distorted expectation using a distortion risk measure for some continuous monotonic function $h$: $$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\int_{\\infty}^{\\infty} z \\frac{\\partial (h \\circ F(z))}{\\partial z} \\mathop{\\mathrm{d}}z. $$\nLet $\\beta : [0, 1] \\rightarrow [0, 1]$ be a distortion risk measure. \\begin{align} {\\pi}(x) \u0026amp;= \\underset{a}{\\arg\\max} \\int_{0}^{1} F^{-1}(\\tau) (\\frac{\\partial \\beta}{\\partial \\tau} \\circ F\u0026rsquo;(z)) \\mathop{\\mathrm{d}}z \\ \u0026amp;= \\underset{a}{\\arg\\max} \\int_{0}^{1} F^{-1}(\\tau) \\mathop{\\mathrm{d}} \\beta(\\tau) \\ \\end{align}\n$$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\frac{1}{N} \\sum_i Q_{\\theta}(x, a; \\beta(\\tau_i)) $$\nExample of a risk-averse policy: CVaR (Chow \u0026amp; Ghavamzadeh, 2014) $$ \\mathrm{CVaR}(\\eta, \\tau) = \\eta \\tau $$ $$ ~{\\pi}(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[\\eta z] $$\nEvaluating under different distortion risk measures is equivalent to changing the sampling distribution for $\\tau$ , allowing us to achieve various forms of risk-sensitive policies. Implementation Architecture Convolutional layers $\\psi: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ Fuly-connected layers $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$ $Q(x,a) \\approx f(\\psi(x))_a$ Embedding for the sample $\\tau$: $\\phi(\\tau): [0,1] \\rightarrow \\mathbb{R}^d$ $$\\phi_j(\\tau):= \\mathrm{ReLU}(\\sum_{i=0}^{n-1}\\cos(\\pi i \\tau)w_{ij} + b_j)$$ i: hidden layer size of the embedding: $n = 64$ j: output size of the embedding: $d$ $Q(x,a; \\tau) \\approx f(\\psi(x) \\odot \\phi(\\tau))_a$ the encoding of $x$ and embedding for $\\tau$ are flexible, which should be experimented. References Distributional Reinforcement Learning for Risk-Sensitive Policies, Shiau Hong Lim and Ilyas Malik, NeurIPS 2022\nCited as:\nGuo, Rong. (November 2022). Risk-sensitive Distributional Reinforcement Learning. Rong’Log. https://rongrg.github.io/posts/2022-11-11-riskrl/.\nOr\n@article{guo2022riskRL, title = \u0026#34;Risk-sensitive Distributional Reinforcement Learning\u0026#34;, author = \u0026#34;Guo, Rong\u0026#34;, journal = \u0026#34;rongrg.github.io\u0026#34;, year = \u0026#34;2022\u0026#34;, month = \u0026#34;November\u0026#34;, url = \u0026#34;https://rongrg.github.io/posts/2022-11-11-riskrl//\u0026#34; } ","permalink":"https://rongrg.github.io/posts/2022-11-11-riskrl/","summary":"What is risk? Generally, risk might arise whenever there is uncertainty. In a financial situation, investment risk can be identified with uncertain monetary loss. In a safety-critical engineering system, risk is the undesirable detrimental outcome.\nHow do we measure risk? Risk metrics typically involve probability distribution of outcomes and quantified loss for each outcome.\nThe standard deviation is a measure of uncertainty, but not a good choice for risk measure.","title":"Risk-sensitive Distributional Reinforcement Learning"}]