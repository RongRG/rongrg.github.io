[{"content":" Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].\nSafety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.\nThree fundamental categories of safe RL algorithms Constrained MDP and Risk-sensitive MDP These algorithms transform the optimization criterion to include some risk measures. This category includes Constrained Markov Decision Process (CMDP) and Risk-Sensitive Markov Decision Process (RS-MDP).\nSafe Exploration This category modifies the exploration process through the incorporation of prior/external knowledge and/or the use of a risk metric, while the optimization criterion remains. Some examples of algorithms in this category are Upper Confidence Bound for Risk (UCBR) and Variance-Based Risk-Sensitive Exploration (VB-RSE).\nAdversary Policy In an RL setting, perturbed observations can fool an agent with a neural network policy into taking actions that lead to poor performance. Adversarial policies can help in detecting and mitigating such attacks.\nOur Journey of Reimplementing Safe RL Algorithms Reimplementing state-of-the-art RL algorithms allows us to gain a deeper understanding of the algorithms\u0026rsquo; inner workings, and subsequently, explore novel and innovative approaches. In my course \u0026ldquo;Advanced Topics in Reinforcement Learning\u0026rdquo;, we took on the challenge of reimplementing ideas from several recent safe RL papers. Our findings and discussions are available as scientific blogs, with code re-implementations available on our GitHub repository (https://github.com/Safe-RL-Team). Join us on an exciting journey of advancing the field of Safe RL!\nSafe Reinforcement Learning via Curriculum Induction, Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal\nðŸ“š Blog Marvin Sextro, Jonas Loos\nSafe Reinforcement Learning with Natural Language Constraints, Tsung-Yen Yang, Michael Hu, Yinlam Chow, Peter J. Ramadge, and Karthik Narasimhan, NeurIPS 2021\nðŸ“š Blog Hongyou Zhou\nAdversarial Policies: Attacking Deep Reinforcement Learning, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, ICLR 2020\nðŸ“š Blog Lorenz Hufe, Jarek Liesen\nReward constrained policy optimization, Chen Tessler, Daniel J. Mankowitz, and Shie Mannor, ICLR 2019\nðŸ“š Blog Boris Meinardus, Tuan Anh Le\nConstrained Policy Optimization via Bayesian World Models, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, ICLR 2022\nðŸ“š Blog Vincent Meilinger\nConstrained Policy Optimization, Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel, ICML 2017\nðŸ“š Blog Thanh Cuong Le, Paul Hasenbusch\nResponsive Safety in Reinforcement Learning by PID Lagrangian Methods Adam, Adam Stooke, Joshua Achiam, and Pieter Abbeel, ICML 2020\nðŸ“š Blog Wenxi Huang\nThere Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning, Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist, NeurIPS 2021\nðŸ“š Blog Malik-Manel Hashim\nUncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble, Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song, NeurIPS 2021\nðŸ“š Blog Jonas Loos, Julian Dralle\nLearning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations, Yuping Luo, and Tengyu Ma, NeurIPS 2021\nðŸ“š Blog Lars Chen, Jeremiah Flannery\nTeachable Reinforcement Learning via Advice Distillation, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, and Abhishek Gupta, NeurIPS 2021\nðŸ“š Blog Mihai Dumitrescu, Claire Sturgill\nCautious Adaptation For Reinforcement Learning in Safety-Critical Settings, Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman, ICML 2020\nðŸ“š Blog Maren Eberle\nVerifiable Reinforcement Learning via Policy Extraction, Osbert Bastani, Yewen Pu, and Armando Solar-Lezama, NeurIPS 2018\nðŸ“š Blog Christoph PrÃ¶schel\nTime Discretization-Invariant Safe Action Repetition for Policy Gradient Methods, Seohong Park, Jaekyeom Kim, and Gunhee Kim, NeurIPS 2021\nðŸ“š Blog Hristo Boyadzhiev\nPushing Boundaries and Prioritizing Safety in RL Safe Reinforcement Learning is a cutting-edge field that holds immense potential for real-world applications. By implementing and exploring ideas from state-of-the-art papers, we can push the boundaries of what is possible and pave the way for even more effective and robust safe RL algorithms.\nSo, let\u0026rsquo;s dive in and make the world a safer place, one policy at a time!\nCitation Cited as:\nGuo, Rong. (April 2023). Safe Reinforcement Learning. Rongâ€™Log. https://rongrg.github.io/posts/2023-04-12-saferl/.\nOr\n@article{guo2023safeRL, title = \u0026#34;Safe Reinforcement Learning\u0026#34;, author = \u0026#34;Guo, Rong\u0026#34;, journal = \u0026#34;rongrg.github.io\u0026#34;, year = \u0026#34;2023\u0026#34;, month = \u0026#34;April\u0026#34;, url = \u0026#34;https://rongrg.github.io/posts/2023-04-12-saferl//\u0026#34; } Reference GarcÃ­a, J. and Fernandez, F., A Comprehensive Survey on Safe Reinforcement Learning Journal of Machine Learning Research, 2015 Ray, A., Achiam, J. and Amodei, D., Benchmarking Safe Exploration in Deep Reinforcement Learning Open AI, 2019 Kumar, A. and Levine, S, Offline Reinforcement Learning: From Algorithms to Practical Challenges NeurIPS Tutorial 2020 ","permalink":"https://rongrg.github.io/posts/2023-04-12-saferl/","summary":"Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].\nSafety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.","title":"Safe Reinforcement Learning"},{"content":" LSTM is a type of recurrent neural network that is widely used in natural language processing, speech recognition, and other applications where sequential data is important. LSTMs are particularly effective at capturing long-term dependencies in sequences of data, which can be challenging for other types of neural networks.\nNavigating the jargon associated with the components of LSTM networks can be daunting, even for those familiar with neural networks. Terms like \u0026ldquo;cell,\u0026rdquo; \u0026ldquo;layer,\u0026rdquo; \u0026ldquo;unit,\u0026rdquo; and \u0026ldquo;neuron\u0026rdquo; are often thrown around without a clear explanation of their meaning and purpose. However, understanding these components is crucial for building and fine-tuning LSTM models. So what exactly are these terms, and how do they benefit the network?\nðŸŒŸ \u0026ldquo;unit\u0026rdquo; = \u0026ldquo;neuron\u0026rdquo;\nðŸŒŸ \u0026ldquo;cell\u0026rdquo; = a collection of \u0026ldquo;neurons\u0026rdquo;\nðŸŒŸ \u0026ldquo;layer\u0026rdquo; = a stack of \u0026ldquo;cells\u0026rdquo;\nAn LSTM Neuron The colah blog shows the core idea behind LSTMs, nicely illustrating the 4 gates inside an LSTM neuron.\nFig. 1. LSTM neuron. (Image source: colah blog) The number of units defines the dimension of the hidden states, which is the same as that of the cell state.\nLet $h$ denote the length of hidden states \u0026ndash; the number of hidden units, which is called hidden_size in PyTorch and num_units in Tensorflow.\nAn LSTM Cell Here is an example LSTM cell that contains $h=2$ LSTM neurons.\nFig. 2. LSTM cell. (Image source: Raimi Karim) The 4 gates in an LSTM neuron suggest that an LSTM neuron has 4 feed-forward neural networks. If we have a sample input of feature size $i$ (e.g., $i=3$ in Fig. 2), we concatenate the sample input with the hidden state and then pass them ($i+h$) together to the gates. The number of parameters in this LSTM cell can be calculated as: $$ 4 \\times [h(h+i) + h] $$\nThe \u0026ldquo;layer\u0026rdquo; is a collection of cells that are stacked on top of each other to form the architecture of the network.\nSo take on the challenge and unlock the full potential of LSTMs!\n","permalink":"https://rongrg.github.io/posts/2023-02-10-lstm/","summary":"LSTM is a type of recurrent neural network that is widely used in natural language processing, speech recognition, and other applications where sequential data is important. LSTMs are particularly effective at capturing long-term dependencies in sequences of data, which can be challenging for other types of neural networks.\nNavigating the jargon associated with the components of LSTM networks can be daunting, even for those familiar with neural networks. Terms like \u0026ldquo;cell,\u0026rdquo; \u0026ldquo;layer,\u0026rdquo; \u0026ldquo;unit,\u0026rdquo; and \u0026ldquo;neuron\u0026rdquo; are often thrown around without a clear explanation of their meaning and purpose.","title":"Understanding LSTM Networks: An Overview of Layers, Cells, Neurons, and Units"},{"content":" Reinforcement learning (RL) is a powerful framework for training agents to maximize cumulative reward, but it typically assumes risk-neutrality. This can lead to suboptimal behavior in practical scenarios where the consequences of unfavorable outcomes can be detrimental.\nWhat is risk? Generally, risk might arise whenever there is uncertainty. In a financial situation, investment risk can be identified with uncertain monetary loss. In a safety-critical engineering system, risk is the undesirable detrimental outcome.\nHow do we measure risk? Risk metrics typically involve probability distribution of outcomes and quantified loss for each outcome.\nThe standard deviation is a measure of uncertainty, but not a good choice for risk measure. Risk concerns only losses, but standard deviation can\u0026rsquo;t account for such asymmetric nature.\nArtzner et. al. (1998) provided an axiomatic definition that a risk measure should satisfy, which they call coherent risk measure. They denote the risk measure by the functional $\\rho(X)$, which assigns a real-valued number to a random variable $X$. $X$ can be thought of as payoffs or returns.\nMonotonicity: $\\rho(Y) \\leq \\rho(X)$ if $Y \\geq X$. Positive Homogeneity: $\\rho(0) = 0, \\rho(\\lambda X) = \\lambda \\rho(X)$ for all $X$ and all $\\lambda \u0026gt; 0$. Sub-additivity: $\\rho(X+Y) \\leq \\rho(X) + \\rho(Y)$ for all $X$ and $Y$. Translation invariance: $\\rho(X+Y) = \\rho(X) -C$ for all $X$ and $C \\in \\mathbb{R}$. The positive Homogeneity and sub-additivity imply that the functional is convex, $$ \\rho(\\lambda X+ (1-\\lambda)Y) \\leq \\rho(\\lambda X) + \\rho((1-\\lambda)Y) = \\lambda \\rho(X) + (1-\\lambda) \\rho(Y) $$ where $\\lambda \\in [0,1]$. This convexity property supports the diversification effect: The risk of a portfolio is no greater than the sum of the risks of its constituents.\nThe translation invariance shows the minimum amount of capital requirements to hedge the risk $\\rho(X)$, $\\rho(X+\\rho(X)) = 0$.\nA commonly used coherent risk measure is the expected shortfall, also known as the conditional value at risk, i.e., CVaR. See also other risk measures: Value at risk (VaR), Entropic value at risk (EVaR), etc.\nRisk-sensitive reinforcement learning Quantiles Let $Z$ be a continuous random variable with a probability density $f$, the cumulative distribution function $F$, and $\\tau \\in (0, 1)$, then the $Ï„$-th quantile of $Z$, $z_{\\tau}$ is the inverse of its c.d.f. $$z_{\\tau} = F^{-1}(\\tau), \\tau = F(z_{\\tau}) = \\int_{-\\infty}^{z_{\\tau}}f(z)\\mathop{\\mathrm{d}}z.$$ We have $$\\tau = F(z), \\mathop{\\mathrm{d}}\\tau = F\u0026rsquo;(z)\\mathop{\\mathrm{d}}z = f(z)\\mathop{\\mathrm{d}}z $$ $$ \\int_0^1 F^{-1}(\\tau)\\mathop{\\mathrm{d}}\\tau = \\int_{\\infty}^{\\infty}F^{-1}(F(z))F\u0026rsquo;(z)\\mathop{\\mathrm{d}}z = \\int_{\\infty}^{\\infty} zf(z)\\mathop{\\mathrm{d}}z = \\mathbb{E}(z) $$\nQuantile Regression sklearn.linear_model.QuantileRegressor is a method for estimating $F^{-1}(\\tau)$\nAs a linear model, the QuantileRegressor gives linear predictions $\\hat{z}(w, X) = Xw$ for the $\\tau$-th quantile ($X:= (x,a)$). The weights $w$ are then found by the following minimization problem: $$\\min_{w} {\\frac{1}{n_{\\text{samples}}} \\sum_i PB_{\\tau}(z_i - X_i w) + \\alpha ||w||_1}. $$ The quantile regression loss (a.k.a pinball loss or linear loss) is\n$$ PB_{\\tau}(u) = |\\tau - \\mathbb{I}_{u \\leq 0}|u = \\tau \\max(u, 0) + (1 - \\tau) \\max(-u, 0) = \\begin{cases} \\tau u, \u0026amp; u \u0026gt; 0, \\\\ 0, \u0026amp; u = 0, \\\\ (\\tau-1) u, \u0026amp; u \u0026lt; 0 \\end{cases} $$\nImplicit Quantile Network is a deterministic neural network parameterizes $(x, a, \\tau)$ and output the quantile of the target distribution $z_{\\tau} = F^{-1}(\\tau) \\approx Q_{\\theta} (x, a; \\tau)$\nThe IQN loss minimizes sampled errors for $Z$ at different $\\tau$s:\nSample $\\tau_i \\sim U([0,1])$ for $N$ times, $i = 1, \\dots, N$ $\\mathcal{L}(x) = \\frac{1}{N}\\sum_i PB_{\\tau_i}(z_{\\tau_i} - Q_{\\theta} (x, a; \\tau_i))$ (If the entire target distribution $Z$ is not accessible, $z_{\\tau_i}$ is replaced with a single target scalar z as an approximation.) Instead of the PB loss, Dabney et al., 2018 used the Huber quantile regression loss with threshold $\\kappa$:\n$$ \\rho_{\\tau}^{\\kappa}(u)= \\begin{cases} |\\tau - \\mathbb{I}_{u \\leq 0}|\\frac{u^2}{2\\kappa}, \u0026amp; |u| \\leq \\kappa, \\\\ |\\tau - \\mathbb{I}_{u \\leq 0}|(|u|-\\frac{1}{2}\\kappa), \u0026amp; |u| \u0026gt; \\kappa \\end{cases} $$\nPolicy The advantage of estimating $Z$ through randomly sampled $\\tau$: risk-sensitive policy.\nrisk refers to the uncertainty over possible outcomes and risk-sensitive policies are those which depend upon more than the mean of the outcomes. Here, the uncertainty is captured by the distribution over returns. Let\u0026rsquo;s take a simple illustrative risk-sensitive criterion Javier GarÄ‡Ä±a and Fernando FerÅ„andez, 2015: $$ \\max_{\\pi}(\\mathbb{E}(z) - \\beta \\mathrm{Var}(z)) $$ Risk-neutral policy $$ \\pi(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[z] = \\frac{1}{N}\\sum_i Q_{\\theta}(x, a; \\tau_i) $$\nRisk-sensitive Policy $$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[U(z)], $$ which is equivalent to maximizing a distorted expectation using a distortion risk measure for some continuous monotonic function $h$: $$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\int_{\\infty}^{\\infty} z \\frac{\\partial (h \\circ F(z))}{\\partial z} \\mathop{\\mathrm{d}}z. $$\nLet $\\beta : [0, 1] \\rightarrow [0, 1]$ be a distortion risk measure.\n$$ \\begin{aligned} {\\pi}(x) \u0026amp;= \\underset{a}{\\arg\\max} \\int_{0}^{1} F^{-1}(\\tau) (\\frac{\\partial \\beta}{\\partial \\tau} \\circ F\u0026rsquo;(z)) \\mathop{\\mathrm{d}}z \\\\ \u0026amp;= \\underset{a}{\\arg\\max} \\int_{0}^{1} F^{-1}(\\tau) \\mathop{\\mathrm{d}} \\beta(\\tau) \\end{aligned} $$\n$$ {\\pi}(x) = \\underset{a}{\\arg\\max} \\frac{1}{N} \\sum_i Q_{\\theta}(x, a; \\beta(\\tau_i)) $$\nExample of a risk-averse policy: CVaR (Chow \u0026amp; Ghavamzadeh, 2014) $$ \\mathrm{CVaR}(\\eta, \\tau) = \\eta \\tau $$ $$ ~{\\pi}(x) = \\underset{a}{\\arg\\max} \\mathbb{E}[\\eta z] $$\nEvaluating under different distortion risk measures is equivalent to changing the sampling distribution for $\\tau$ , allowing us to achieve various forms of risk-sensitive policies. Implementation Architecture Convolutional layers $\\psi: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ Fuly-connected layers $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$ $Q(x,a) \\approx f(\\psi(x))_a$ Embedding for the sample $\\tau$: $\\phi(\\tau): [0,1] \\rightarrow \\mathbb{R}^d$ $$\\phi_j(\\tau):= \\mathrm{ReLU}(\\sum_{i=0}^{n-1}\\cos(\\pi i \\tau)w_{ij} + b_j)$$ i: hidden layer size of the embedding: $n = 64$ j: output size of the embedding: $d$ $Q(x,a; \\tau) \\approx f(\\psi(x) \\odot \\phi(\\tau))_a$ the encoding of $x$ and embedding for $\\tau$ are flexible, which should be experimented. Citation Cited as:\nGuo, Rong. (November 2022). Risk-sensitive Distributional Reinforcement Learning. Rongâ€™Log. https://rongrg.github.io/posts/2022-11-11-riskrl/.\nOr\n@article{guo2022riskRL, title = \u0026#34;Risk-sensitive Distributional Reinforcement Learning\u0026#34;, author = \u0026#34;Guo, Rong\u0026#34;, journal = \u0026#34;rongrg.github.io\u0026#34;, year = \u0026#34;2022\u0026#34;, month = \u0026#34;November\u0026#34;, url = \u0026#34;https://rongrg.github.io/posts/2022-11-11-riskrl//\u0026#34; } Reference Will Dabney, et al. Implicit Quantile Networks for Distributional Reinforcement Learning ICML 2018 Christian Bodnar, et al. Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping, RSS 2020 Shiau Hong Lim, et al. Distributional Reinforcement Learning for Risk-Sensitive Policies, NeurIPS 2022 ","permalink":"https://rongrg.github.io/posts/2022-11-11-riskrl/","summary":"Reinforcement learning (RL) is a powerful framework for training agents to maximize cumulative reward, but it typically assumes risk-neutrality. This can lead to suboptimal behavior in practical scenarios where the consequences of unfavorable outcomes can be detrimental.\nWhat is risk? Generally, risk might arise whenever there is uncertainty. In a financial situation, investment risk can be identified with uncertain monetary loss. In a safety-critical engineering system, risk is the undesirable detrimental outcome.","title":"Risk-sensitive Distributional Reinforcement Learning"},{"content":"Setting up an ML envronment can be a tricky thing. Here\u0026rsquo;s what worked for me on how to set up the environment and keep track of experiments.\nProject Setup Directory Structure Cookiecuttet\nComputational Environment Pycharm virtualenv pip install the packages: [1] Unzip the downloaded mjpro150 into ~/.mujoco/mjpro150, and place the mjkey.txt file at ~/.mujoco/mjkey.txt.\n[2] Run pip3 install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[3] Remove ~/.mujoco/mjpro150/bin/libglfw.3.dylib\n[4] Run brew install llvm boost hdf5 glfw\n[5] Add\nexport PATH=\u0026#34;/usr/local/opt/llvm/bin:$PATH\u0026#34; export CC=\u0026#34;/usr/local/opt/llvm/bin/clang\u0026#34; export CXX=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX11=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX14=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX17=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export CXX1X=\u0026#34;/usr/local/opt/llvm/bin/clang++\u0026#34; export LDFLAGS=\u0026#34;-L/usr/local/opt/llvm/lib\u0026#34; export CPPFLAGS=\u0026#34;-I/usr/local/opt/llvm/include\u0026#34; to .zshrc and source ~/.zshrc\n[6] Run pip install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[7] Run python3 -c 'import mujoco_py'\n[8] Run pip install 'gym[all]'\nExperiments Hyperparameters Description Performance on benchmarking tasks Preparing for publishing README Add packages to requirements.txt pip freeze \u0026gt; requirements.txt Source code Visualizations ","permalink":"https://rongrg.github.io/posts/2022-05-24-blog-post-1/","summary":"Setting up an ML envronment can be a tricky thing. Here\u0026rsquo;s what worked for me on how to set up the environment and keep track of experiments.\nProject Setup Directory Structure Cookiecuttet\nComputational Environment Pycharm virtualenv pip install the packages: [1] Unzip the downloaded mjpro150 into ~/.mujoco/mjpro150, and place the mjkey.txt file at ~/.mujoco/mjkey.txt.\n[2] Run pip3 install -U 'mujoco-py\u0026lt;1.50.2,\u0026gt;=1.50.1'\n[3] Remove ~/.mujoco/mjpro150/bin/libglfw.3.dylib\n[4] Run brew install llvm boost hdf5 glfw","title":"How to Set Up an RL Project Less Complicated?"},{"content":"","permalink":"https://rongrg.github.io/content/_index.md","summary":"","title":""},{"content":"Summer 2018, Summer 2022, Winter 2022\nThere are many great online resources available for learning about reinforcement learning. Here are a few to get you started.\nA (Long) Peek into Reinforcement Learning\nSpinning Up in Deep RL\nDavid Silverâ€™s RL course at UCL\nBooks\nSutton \u0026amp; Bartoâ€™s RL book\nDimitri P. Bertsekas\u0026rsquo; books\nCoding\nCoding PPO from Scratch with PyTorch\nThe 37 Implementation Details of Proximal Policy Optimization\nSeminars\nHere is a compilation of recent machine learning papers focused on safe reinforcement learning, spanning from 2017 to 2022.\nSuggestions for preparing a seminar presentation\nHow to Speak by Patrick Winston Presenting at Journal Club by Constantinos Timinis \u0026amp; Simone Foti How to Create a Distill Article? ","permalink":"https://rongrg.github.io/teaching/2022-winter-teaching-4/","summary":"Summer 2018, Summer 2022, Winter 2022\nThere are many great online resources available for learning about reinforcement learning. Here are a few to get you started.\nA (Long) Peek into Reinforcement Learning\nSpinning Up in Deep RL\nDavid Silverâ€™s RL course at UCL\nBooks\nSutton \u0026amp; Bartoâ€™s RL book\nDimitri P. Bertsekas\u0026rsquo; books\nCoding\nCoding PPO from Scratch with PyTorch\nThe 37 Implementation Details of Proximal Policy Optimization\nSeminars\nHere is a compilation of recent machine learning papers focused on safe reinforcement learning, spanning from 2017 to 2022.","title":"Advanced Topics in Reinforcement Learning"},{"content":"Summer 2019, Summer 2020, Summer 2021, Summer 2022\n","permalink":"https://rongrg.github.io/teaching/2019-summer-teaching-2/","summary":"Summer 2019, Summer 2020, Summer 2021, Summer 2022","title":"Data Structure and Algorithms"},{"content":"Winter 2018, Winter 2019, Winter 2020, Winter 2021\n","permalink":"https://rongrg.github.io/teaching/2018-winter-teaching-1/","summary":"Winter 2018, Winter 2019, Winter 2020, Winter 2021","title":"Introduction to Computer Science and Programming"},{"content":" Risk Sensitivity under Partially Observable Markov Decision Processes\nNikolas HÃ¶ft, Rong Guo, Vaios Laschos, Sein Jeung, Dirk Ostwald und Klaus Obermayer\nConference on Cognitive Computational Neuroscience (CCN), 2019 paper\nInteraction of instrumental and goal-directed learning modulates prediction error representations in the ventral striatum\nRong Guo, Wendelin BÃ¶hmer, Martin Hebart, Samson Chien, Tobias Sommer, Klaus Obermayer and Jan GlÃ¤scher\nJournal of Neuroscience, 2016 paper\nAltered behavioral and neural responsiveness to counterfactual gains in the elderly\nMichael J Tobia, Rong Guo, Jan GlÃ¤scher, Ulrike Schwarze, Stefanie Brassen, Christian BÃ¼chel, Klaus Obermayer and Tobias Sommer\nCognitive, Affective, \u0026amp; Behavioral Neuroscience, 2016 paper\nNon-deterministic policy improvement stabilizes approximated reinforcement learning\nWendelin BÃ¶hmer, Rong Guo and Klaus Obermayer\nEuropean Workshop on Reinforcement Learning (EWRL), 2016 paper\nNeural systems for choice and valuation with counterfactual learning signals\nMichael J Tobia (co-first author), Rong Guo (co-first author), Ulrike Schwarze, Wendelin BÃ¶hmer, Jan GlÃ¤scher, Barbara Finckh, Andreas Marschner, Christian BÃ¼chel, Klaus Obermayer and Tobias Sommer NeuroImage, 2014 paper\nThis listing can also be found on my Google Scholar profile.\n","permalink":"https://rongrg.github.io/publications/","summary":"Risk Sensitivity under Partially Observable Markov Decision Processes\nNikolas HÃ¶ft, Rong Guo, Vaios Laschos, Sein Jeung, Dirk Ostwald und Klaus Obermayer\nConference on Cognitive Computational Neuroscience (CCN), 2019 paper\nInteraction of instrumental and goal-directed learning modulates prediction error representations in the ventral striatum\nRong Guo, Wendelin BÃ¶hmer, Martin Hebart, Samson Chien, Tobias Sommer, Klaus Obermayer and Jan GlÃ¤scher\nJournal of Neuroscience, 2016 paper\nAltered behavioral and neural responsiveness to counterfactual gains in the elderly","title":"Publications"}]