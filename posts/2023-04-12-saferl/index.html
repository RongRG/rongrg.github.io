<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Safe Reinforcement Learning | Rong&#39;s Homepage</title>
<meta name="keywords" content="reinforcement learning, safety">
<meta name="description" content="Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].
Safety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.">
<meta name="author" content="">
<link rel="canonical" href="https://rongrg.github.io/posts/2023-04-12-saferl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dc96e9e0118e5e264a03d68b104df6ae869cfb73c61f5f89dd91aeb16b0d8c03.css" integrity="sha256-3Jbp4BGOXiZKA9aLEE32roac&#43;3PGH1&#43;J3ZGusWsNjAM=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://rongrg.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://rongrg.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://rongrg.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://rongrg.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://rongrg.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
onload="renderMathInElement(document.body);"></script>


<meta property="og:title" content="Safe Reinforcement Learning" />
<meta property="og:description" content="Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].
Safety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rongrg.github.io/posts/2023-04-12-saferl/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-04-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Safe Reinforcement Learning"/>
<meta name="twitter:description" content="Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].
Safety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://rongrg.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Safe Reinforcement Learning",
      "item": "https://rongrg.github.io/posts/2023-04-12-saferl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Safe Reinforcement Learning",
  "name": "Safe Reinforcement Learning",
  "description": "Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].\nSafety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.",
  "keywords": [
    "reinforcement learning", "safety"
  ],
  "articleBody": " Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].\nSafety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.\nThree fundamental categories of safe RL algorithms Constrained MDP and Risk-sensitive MDP These algorithms transform the optimization criterion to include some risk measures. This category includes Constrained Markov Decision Process (CMDP) and Risk-Sensitive Markov Decision Process (RS-MDP).\nSafe Exploration This category modifies the exploration process through the incorporation of prior/external knowledge and/or the use of a risk metric, while the optimization criterion remains. Some examples of algorithms in this category are Upper Confidence Bound for Risk (UCBR) and Variance-Based Risk-Sensitive Exploration (VB-RSE).\nAdversary Policy In an RL setting, perturbed observations can fool an agent with a neural network policy into taking actions that lead to poor performance. Adversarial policies can help in detecting and mitigating such attacks.\nOur Journey of Reimplementing Safe RL Algorithms Reimplementing state-of-the-art RL algorithms allows us to gain a deeper understanding of the algorithms‚Äô inner workings, and subsequently, explore novel and innovative approaches. In my course ‚ÄúAdvanced Topics in Reinforcement Learning‚Äù, we took on the challenge of reimplementing ideas from several recent safe RL papers. Our findings and discussions are available as scientific blogs, with code re-implementations available on our GitHub repository (https://github.com/Safe-RL-Team). Join us on an exciting journey of advancing the field of Safe RL!\nSafe Reinforcement Learning via Curriculum Induction, Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal\nüìö Blog Marvin Sextro, Jonas Loos\nSafe Reinforcement Learning with Natural Language Constraints, Tsung-Yen Yang, Michael Hu, Yinlam Chow, Peter J. Ramadge, and Karthik Narasimhan, NeurIPS 2021\nüìö Blog Hongyou Zhou\nAdversarial Policies: Attacking Deep Reinforcement Learning, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, ICLR 2020\nüìö Blog Lorenz Hufe, Jarek Liesen\nReward constrained policy optimization, Chen Tessler, Daniel J. Mankowitz, and Shie Mannor, ICLR 2019\nüìö Blog Boris Meinardus, Tuan Anh Le\nConstrained Policy Optimization via Bayesian World Models, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, ICLR 2022\nüìö Blog Vincent Meilinger\nConstrained Policy Optimization, Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel, ICML 2017\nüìö Blog Thanh Cuong Le, Paul Hasenbusch\nResponsive Safety in Reinforcement Learning by PID Lagrangian Methods Adam, Adam Stooke, Joshua Achiam, and Pieter Abbeel, ICML 2020\nüìö Blog Wenxi Huang\nThere Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning, Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist, NeurIPS 2021\nüìö Blog Malik-Manel Hashim\nUncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble, Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song, NeurIPS 2021\nüìö Blog Jonas Loos, Julian Dralle\nLearning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations, Yuping Luo, and Tengyu Ma, NeurIPS 2021\nüìö Blog Lars Chen, Jeremiah Flannery\nTeachable Reinforcement Learning via Advice Distillation, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, and Abhishek Gupta, NeurIPS 2021\nüìö Blog Mihai Dumitrescu, Claire Sturgill\nCautious Adaptation For Reinforcement Learning in Safety-Critical Settings, Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman, ICML 2020\nüìö Blog Maren Eberle\nVerifiable Reinforcement Learning via Policy Extraction, Osbert Bastani, Yewen Pu, and Armando Solar-Lezama, NeurIPS 2018\nüìö Blog Christoph Pr√∂schel\nTime Discretization-Invariant Safe Action Repetition for Policy Gradient Methods, Seohong Park, Jaekyeom Kim, and Gunhee Kim, NeurIPS 2021\nüìö Blog Hristo Boyadzhiev\nPushing Boundaries and Prioritizing Safety in RL Safe Reinforcement Learning is a cutting-edge field that holds immense potential for real-world applications. By implementing and exploring ideas from state-of-the-art papers, we can push the boundaries of what is possible and pave the way for even more effective and robust safe RL algorithms.\nSo, let‚Äôs dive in and make the world a safer place, one policy at a time!\nCitation Cited as:\nGuo, Rong. (April 2023). Safe Reinforcement Learning. Rong‚ÄôLog. https://rongrg.github.io/posts/2023-04-12-saferl/.\nOr\n@article{guo2023safeRL, title = \"Safe Reinforcement Learning\", author = \"Guo, Rong\", journal = \"rongrg.github.io\", year = \"2023\", month = \"April\", url = \"https://rongrg.github.io/posts/2023-04-12-saferl//\" } Reference Garc√≠a, J. and Fernandez, F., A Comprehensive Survey on Safe Reinforcement Learning Journal of Machine Learning Research, 2015 Ray, A., Achiam, J. and Amodei, D., Benchmarking Safe Exploration in Deep Reinforcement Learning Open AI, 2019 Kumar, A. and Levine, S, Offline Reinforcement Learning: From Algorithms to Practical Challenges NeurIPS Tutorial 2020 ",
  "wordCount" : "764",
  "inLanguage": "en",
  "datePublished": "2023-04-12T00:00:00Z",
  "dateModified": "2023-04-12T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rongrg.github.io/posts/2023-04-12-saferl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rong's Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rongrg.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://rongrg.github.io" accesskey="h" title="Rong&#39;s Homepage (Alt + H)">Rong&#39;s Homepage</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://rongrg.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://rongrg.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://rongrg.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://rongrg.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://rongrg.github.io/teaching/" title="Teaching">
                    <span>Teaching</span>
                </a>
            </li>
            <li>
                <a href="https://rongrg.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://rongrg.github.io">Home</a>&nbsp;¬ª&nbsp;<a href="https://rongrg.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Safe Reinforcement Learning
    </h1>
    <div class="post-meta"><span title='2023-04-12 00:00:00 +0000 UTC'>April 12, 2023</span>&nbsp;¬∑&nbsp;4 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#balancing-performance-and-safety-in-rl">Balancing Performance and Safety in RL</a>
      <ul>
        <li><a href="#three-fundamental-categories-of-safe-rl-algorithms">Three fundamental categories of safe RL algorithms</a></li>
      </ul>
    </li>
    <li><a href="#our-journey-of-reimplementing-safe-rl-algorithms">Our Journey of Reimplementing Safe RL Algorithms</a></li>
    <li><a href="#pushing-boundaries-and-prioritizing-safety-in-rl">Pushing Boundaries and Prioritizing Safety in RL</a></li>
    <li><a href="#citation">Citation</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
<h2 id="balancing-performance-and-safety-in-rl">Balancing Performance and Safety in RL<a hidden class="anchor" aria-hidden="true" href="#balancing-performance-and-safety-in-rl">#</a></h2>
<p>Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only
maximize the long-term reward but also ensure reasonable system performance and/or respect safety
constraints during the learning and/or deployment processes [1].</p>
<p>Safety is the opposite of risk, which refers to the stochastic nature of the environment.
An optimal policy for long-term reward maximization may still perform poorly in some catastrophic
situations due to inherent uncertainty.</p>
<h3 id="three-fundamental-categories-of-safe-rl-algorithms">Three fundamental categories of safe RL algorithms<a hidden class="anchor" aria-hidden="true" href="#three-fundamental-categories-of-safe-rl-algorithms">#</a></h3>
<ol>
<li>Constrained MDP and Risk-sensitive MDP</li>
</ol>
<p>These algorithms transform the optimization criterion to include some risk measures.
This category includes Constrained Markov Decision Process (CMDP) and Risk-Sensitive Markov Decision Process (RS-MDP).</p>
<ol start="2">
<li>Safe Exploration</li>
</ol>
<p>This category modifies the exploration process through the incorporation of prior/external knowledge and/or the use of a risk metric,
while the optimization criterion remains.
Some examples of algorithms in this category are Upper Confidence Bound for Risk (UCBR) and Variance-Based Risk-Sensitive Exploration (VB-RSE).</p>
<ol start="3">
<li>Adversary Policy</li>
</ol>
<p>In an RL setting, perturbed observations can fool an agent with a neural network policy into taking actions that lead to poor performance.
Adversarial policies can help in detecting and mitigating such attacks.</p>
<h2 id="our-journey-of-reimplementing-safe-rl-algorithms">Our Journey of Reimplementing Safe RL Algorithms<a hidden class="anchor" aria-hidden="true" href="#our-journey-of-reimplementing-safe-rl-algorithms">#</a></h2>
<p>Reimplementing state-of-the-art RL algorithms allows us to gain a deeper understanding of the algorithms&rsquo; inner workings, and subsequently,
explore novel and innovative approaches.
In my course <a href="https://rongrg.github.io/teaching/2022-winter-teaching-4/">&ldquo;Advanced Topics in Reinforcement Learning&rdquo;</a>,
we took on the challenge of reimplementing ideas from several recent safe RL papers.
Our findings and discussions are available as scientific blogs,
with code re-implementations available on our GitHub repository (<a href="https://github.com/Safe-RL-Team)">https://github.com/Safe-RL-Team)</a>.
Join us on an exciting journey of advancing the field of Safe RL!</p>
<ol>
<li>
<p><a href="https://arxiv.org/pdf/2006.12136.pdf">Safe Reinforcement Learning via Curriculum Induction</a>, Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal</p>
<p>üìö <a href="https://safe-rl-team.github.io/curriculum-learning/">Blog</a> Marvin Sextro, Jonas Loos</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/file/72f67e70f6b7cdc4cc893edaddf0c4c6-Paper.pdf">Safe Reinforcement Learning with Natural Language Constraints</a>, Tsung-Yen Yang, Michael Hu, Yinlam Chow, Peter J. Ramadge, and Karthik Narasimhan, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/SRL-NLC-Report/">Blog</a>   Hongyou Zhou</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=HJgEMpVFwB">Adversarial Policies: Attacking Deep Reinforcement Learning</a>, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, ICLR 2020</p>
<p>üìö <a href="https://safe-rl-team.github.io/adversarial-policies-pytorch-blog/">Blog</a>   Lorenz Hufe, Jarek Liesen</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=SkfrvsA9FX">Reward constrained policy optimization</a>, Chen Tessler, Daniel J. Mankowitz, and Shie Mannor, ICLR 2019</p>
<p>üìö <a href="https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/">Blog</a>   Boris Meinardus, Tuan Anh Le</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2201.09802.pdf">Constrained Policy Optimization via Bayesian World Models</a>, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, ICLR 2022</p>
<p>üìö <a href="https://safe-rl-team.github.io/lambda-bo-blog/">Blog</a>   Vincent Meilinger</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf">Constrained Policy Optimization</a>, Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel, ICML 2017</p>
<p>üìö <a href="https://safe-rl-team.github.io/CPO-Blog/">Blog</a>   Thanh Cuong Le, Paul Hasenbusch</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v119/stooke20a/stooke20a.pdf">Responsive Safety in Reinforcement Learning by PID Lagrangian Methods Adam</a>, Adam Stooke, Joshua Achiam, and Pieter Abbeel, ICML 2020</p>
<p>üìö <a href="https://safe-rl-team.github.io/PID/">Blog</a>   Wenxi Huang</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=3X65eaS4PtP">There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning</a>, Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/Blog-Post-about-There-is-No-Turning-Back/There%20is%20No%20Turning%20Back%20A%20Self-Supervised%20Approach%20for%20Reversibility-Aware%20Reinforcement%20Learning.html">Blog</a>   Malik-Manel Hashim</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf">Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble</a>, Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/Uncertainty-Based-Offline-RL-with-Diversified-Q-Ensemble/">Blog</a>   Jonas Loos, Julian Dralle</p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=K4Su8BIivap">Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations</a>, Yuping Luo, and Tengyu Ma, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/barrier-certificates/">Blog</a>   Lars Chen, Jeremiah Flannery</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/file/37cfff3c04f95b22bcf166df586cd7a9-Paper.pdf">Teachable Reinforcement Learning via Advice Distillation</a>, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, and Abhishek Gupta, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/advice-distillation-blog/">Blog</a>   Mihai Dumitrescu, Claire Sturgill</p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v119/zhang20e/zhang20e.pdf">Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings</a>, Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman, ICML 2020</p>
<p>üìö <a href="https://safe-rl-team.github.io/CARL/">Blog</a>   Maren Eberle</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1805.08328.pdf">Verifiable Reinforcement Learning via Policy Extraction</a>, Osbert Bastani, Yewen Pu, and Armando Solar-Lezama, NeurIPS 2018</p>
<p>üìö <a href="https://safe-rl-team.github.io/viper-verifiable-reinforcement-learning/">Blog</a>   Christoph Pr√∂schel</p>
</li>
<li>
<p><a href="https://proceedings.neurips.cc/paper/2021/file/024677efb8e4aee2eaeef17b54695bbe-Paper.pdf">Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods</a>, Seohong Park, Jaekyeom Kim, and Gunhee Kim, NeurIPS 2021</p>
<p>üìö <a href="https://safe-rl-team.github.io/safe-action-repetition-article/">Blog</a>   Hristo Boyadzhiev</p>
</li>
</ol>
<h2 id="pushing-boundaries-and-prioritizing-safety-in-rl">Pushing Boundaries and Prioritizing Safety in RL<a hidden class="anchor" aria-hidden="true" href="#pushing-boundaries-and-prioritizing-safety-in-rl">#</a></h2>
<p>Safe Reinforcement Learning is a cutting-edge field that holds immense potential for real-world applications.
By implementing and exploring ideas from state-of-the-art papers, we can push the boundaries of what is possible
and pave the way for even more effective and robust safe RL algorithms.</p>
<p>So, let&rsquo;s dive in and make the world a safer place, one policy at a time!</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>Cited as:</p>
<blockquote>
<p>Guo, Rong. (April 2023). Safe Reinforcement Learning. Rong‚ÄôLog. <a href="https://rongrg.github.io/posts/2023-04-12-saferl/">https://rongrg.github.io/posts/2023-04-12-saferl/</a>.</p>
</blockquote>
<p>Or</p>
<pre tabindex="0"><code>@article{guo2023safeRL,
  title   = &#34;Safe Reinforcement Learning&#34;,
  author  = &#34;Guo, Rong&#34;,
  journal = &#34;rongrg.github.io&#34;,
  year    = &#34;2023&#34;,
  month   = &#34;April&#34;,
  url     = &#34;https://rongrg.github.io/posts/2023-04-12-saferl//&#34;
}
</code></pre><h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ol>
<li><a href="https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf">Garc√≠a, J. and Fernandez, F., A Comprehensive Survey on Safe Reinforcement Learning <em>Journal of Machine Learning Research, 2015</em></a></li>
<li><a href="https://cdn.openai.com/safexp-short.pdf">Ray, A., Achiam, J. and Amodei, D., Benchmarking Safe Exploration in Deep Reinforcement Learning <em>Open AI, 2019</em></a></li>
<li><a href="https://sites.google.com/view/offlinerltutorial-neurips2020/home?authuser=0">Kumar, A. and Levine, S, Offline Reinforcement Learning: From Algorithms to Practical Challenges <em>NeurIPS Tutorial 2020</em></a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://rongrg.github.io/tags/reinforcement-learning/">reinforcement learning</a></li>
      <li><a href="https://rongrg.github.io/tags/safety/">safety</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://rongrg.github.io/posts/2023-02-10-lstm/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Understanding LSTM Networks: An Overview of Layers, Cells, Neurons, and Units</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=Safe%20Reinforcement%20Learning&amp;url=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f&amp;hashtags=reinforcementlearning%2csafety">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f&amp;title=Safe%20Reinforcement%20Learning&amp;summary=Safe%20Reinforcement%20Learning&amp;source=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f&title=Safe%20Reinforcement%20Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on whatsapp"
        href="https://api.whatsapp.com/send?text=Safe%20Reinforcement%20Learning%20-%20https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Safe Reinforcement Learning on telegram"
        href="https://telegram.me/share/url?text=Safe%20Reinforcement%20Learning&amp;url=https%3a%2f%2frongrg.github.io%2fposts%2f2023-04-12-saferl%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://rongrg.github.io">Rong&#39;s Homepage</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
