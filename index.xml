<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Home on Rong&#39;s Homepage</title>
    <link>https://rongrg.github.io/</link>
    <description>Recent content in Home on Rong&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://rongrg.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Safe Reinforcement Learning</title>
      <link>https://rongrg.github.io/posts/2023-04-12-saferl/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/posts/2023-04-12-saferl/</guid>
      <description>Balancing Performance and Safety in RL Safe Reinforcement Learning (RL) is a subset of RL that focuses on learning policies that not only maximize the long-term reward but also ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes [1].
Safety is the opposite of risk, which refers to the stochastic nature of the environment. An optimal policy for long-term reward maximization may still perform poorly in some catastrophic situations due to inherent uncertainty.</description>
    </item>
    
    <item>
      <title>Understanding LSTM Networks: An Overview of Layers, Cells, Neurons, and Units</title>
      <link>https://rongrg.github.io/posts/2023-02-10-lstm/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/posts/2023-02-10-lstm/</guid>
      <description>LSTM is a type of recurrent neural network that is widely used in natural language processing, speech recognition, and other applications where sequential data is important. LSTMs are particularly effective at capturing long-term dependencies in sequences of data, which can be challenging for other types of neural networks.
Navigating the jargon associated with the components of LSTM networks can be daunting, even for those familiar with neural networks. Terms like &amp;ldquo;cell,&amp;rdquo; &amp;ldquo;layer,&amp;rdquo; &amp;ldquo;unit,&amp;rdquo; and &amp;ldquo;neuron&amp;rdquo; are often thrown around without a clear explanation of their meaning and purpose.</description>
    </item>
    
    <item>
      <title>Risk-sensitive Distributional Reinforcement Learning</title>
      <link>https://rongrg.github.io/posts/2022-11-11-riskrl/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/posts/2022-11-11-riskrl/</guid>
      <description>Reinforcement learning (RL) is a powerful framework for training agents to maximize cumulative reward, but it typically assumes risk-neutrality. This can lead to suboptimal behavior in practical scenarios where the consequences of unfavorable outcomes can be detrimental.
What is risk? Generally, risk might arise whenever there is uncertainty. In a financial situation, investment risk can be identified with uncertain monetary loss. In a safety-critical engineering system, risk is the undesirable detrimental outcome.</description>
    </item>
    
    <item>
      <title>How to Set Up an RL Project Less Complicated?</title>
      <link>https://rongrg.github.io/posts/2022-05-24-blog-post-1/</link>
      <pubDate>Tue, 24 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/posts/2022-05-24-blog-post-1/</guid>
      <description>Setting up an ML envronment can be a tricky thing. Here&amp;rsquo;s what worked for me on how to set up the environment and keep track of experiments.
Project Setup Directory Structure Cookiecuttet
Computational Environment Pycharm virtualenv pip install the packages: [1] Unzip the downloaded mjpro150 into ~/.mujoco/mjpro150, and place the mjkey.txt file at ~/.mujoco/mjkey.txt.
[2] Run pip3 install -U &#39;mujoco-py&amp;lt;1.50.2,&amp;gt;=1.50.1&#39;
[3] Remove ~/.mujoco/mjpro150/bin/libglfw.3.dylib
[4] Run brew install llvm boost hdf5 glfw</description>
    </item>
    
    <item>
      <title></title>
      <link>https://rongrg.github.io/content/_index.md</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/content/_index.md</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advanced Topics in Reinforcement Learning</title>
      <link>https://rongrg.github.io/teaching/2022-winter-teaching-4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/teaching/2022-winter-teaching-4/</guid>
      <description>Summer 2018, Summer 2022, Winter 2022
There are many great online resources available for learning about reinforcement learning. Here are a few to get you started.
A (Long) Peek into Reinforcement Learning
Spinning Up in Deep RL
David Silver’s RL course at UCL
Machine Learning
A basic understanding of machine learning is a great first step to dive deeper into the field.
Machine Learning Deep Learning Books
Sutton &amp;amp; Barto’s RL book</description>
    </item>
    
    
    <item>
      <title>Data Structure and Algorithms</title>
      <link>https://rongrg.github.io/teaching/2019-summer-teaching-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/teaching/2019-summer-teaching-2/</guid>
      <description>Summer 2019, Summer 2020, Summer 2021, Summer 2022, Summer 2023</description>
    </item>
    
    <item>
      <title>Introduction to Computer Science and Programming</title>
      <link>https://rongrg.github.io/teaching/2018-winter-teaching-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/teaching/2018-winter-teaching-1/</guid>
      <description>Winter 2018, Winter 2019, Winter 2020, Winter 2021, Winter 2022</description>
    </item>
    
    <item>
      <title>Publications</title>
      <link>https://rongrg.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rongrg.github.io/publications/</guid>
      <description>Risk Sensitivity under Partially Observable Markov Decision Processes
Nikolas Höft, Rong Guo, Vaios Laschos, Sein Jeung, Dirk Ostwald und Klaus Obermayer
Conference on Cognitive Computational Neuroscience (CCN), 2019 paper
Interaction of instrumental and goal-directed learning modulates prediction error representations in the ventral striatum
Rong Guo, Wendelin Böhmer, Martin Hebart, Samson Chien, Tobias Sommer, Klaus Obermayer and Jan Gläscher
Journal of Neuroscience, 2016 paper
Altered behavioral and neural responsiveness to counterfactual gains in the elderly</description>
    </item>
    
    
  </channel>
</rss>
